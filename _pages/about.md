---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm currently a second year PhD student at UCLA, advised by Prof. Kai-Wei Chang. 

My research interest lies in the intersection of Computer Vision (CV) and Natural Language Processing (NLP), aiming to equip computers with the ability to understand and relate data across different modalities. Specifically, I am interested in the following topcis:

- **Representation Learning for Decision Making**: *Vision-and-Language Navigation (VLN)*, *Robotic Manipulation*, *Generative Models for planning*
- **Learning reasoning via Interaction**: *Learning VL representations via embodied interactions*, *Learning relation-aware VL representations from video*
- **Compositionality skills for multimodal generation and reasoning**: *Open-World Image/Video Captioning*, *Language-Conditioned Image Manipulation*

I'm looking for Research Intern opportunities for *Summer 2024*.


# Publications
## 2023
### Planning as In-Painting: A Diffusion-Based Embodied Task Planning Framework for Environments under Uncertainty
<span style='font-size:0.9em'>**Cheng-Fu Yang**, Haoyang Xu, Te-Lin Wu, Xiaofeng Gao, Kai-Wei Chang, Feng Gao</span>
<span style='font-size:0.9em'>*Arxiv 2023*</span>
<div>
  |<a href="https://arxiv.org/abs/2310.12344" target="_blank">paper</a> |
  <a href="https://github.com/joeyy5588/planning-as-inpainting" target="_blank">code</a> |
</div>

### LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following
<span style='font-size:0.9em'>**Cheng-Fu Yang**, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, Yu-Chiang Frank Wang, Kai-Wei Chang</span>
<span style='font-size:0.9em'>*Accepted to EMNLP 2023*</span>
<div>
  |<a href="https://arxiv.org/abs/2310.12344" target="_blank">paper</a> |
  <a href="https://github.com/joeyy5588/LACMA" target="_blank">code</a> |
</div>

## 2022

### Paraphrasing is all you need for Novel Object Captioning
<span style='font-size:0.9em'>**Cheng-Fu Yang**, Yao-Hung Hubert Tsai, Wan-Cyuan Fan, Yu-Chiang Frank Wang, Louis-Philippe Morency, Ruslan Salakhutdinov</span>
<span style='font-size:0.9em'>*Accepted to NeurIPS 2022*</span>
<div>
  |<a href="https://arxiv.org/abs/2209.12343" target="_blank">paper</a> |
  <a href="https://github.com/joeyy5588/P2C" target="_blank">code</a> |
</div>

### Target-free Text-guided Image Manipulation
<span style='font-size:0.9em'>Wan-Cyuan Fan, **Cheng-Fu Yang**, Qiao-An Yang, Yu-Chiang Frank Wang</span>
<span style='font-size:0.9em'>*Accepted to AAAI 2023*</span>
<div>
  |<a href="https://arxiv.org/abs/2211.14544" target="_blank">paper</a> |
</div>

### Scene Graph Expansion for Semantics-Guided Image Completion
<span style='font-size:0.9em'>Qiao-An Yang, **Cheng-Fu Yang**, Wan-Cyuan Fan, Cheng-Yo Tan, Meng-Lin Wu, Yu-Chiang Frank Wang</span>
<span style='font-size:0.9em'>*Accepted to CVPR 2022*</span>
<div>
  |<a href="ttps://arxiv.org/abs/2205.02958" target="_blank">paper</a> |
</div>

### Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation
<span style='font-size:0.9em'>Chih-Chun Yang, **Cheng-Fu Yang**, Wan-Cyuan Fan and Yu-Chiang Frank Wang</span>
<span style='font-size:0.9em'>*Accepted to AAAI 2022*</span>
<div>
  |<a href="https://ojs.aaai.org/index.php/AAAI/article/view/20210" target="_blank">paper</a> |
</div>


## 2021

### LayoutTransformer: Scene Layout Generation with Conceptual and Spatial Diversity
<span style='font-size:0.9em'>**Cheng-Fu Yang\***, Wan-Cyuan Fan\*, Fu-En Yang and Yu-Chiang Frank Wang.</span>
<span style='font-size:0.9em'>*Accepted to CVPR 2021*</span>
<div>
  |<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Yang_LayoutTransformer_Scene_Layout_Generation_With_Conceptual_and_Spatial_Diversity_CVPR_2021_paper.html" target="_blank">paper</a> |
  <a href="https://github.com/davidhalladay/LayoutTransformer" target="_blank">code</a> |
</div>

